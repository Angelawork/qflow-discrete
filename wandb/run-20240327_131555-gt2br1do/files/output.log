/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/wandb/sdk/lib/import_hooks.py:243: DeprecationWarning: Deprecated since Python 3.4. Use importlib.util.find_spec() instead.
  loader = importlib.find_loader(fullname, path)
/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be <class 'numpy.float32'>, actual type: float64
  logger.warn(
/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.
  logger.warn(f"{pre} is not within the observation space.")
/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: [33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be <class 'numpy.float32'>, actual type: float64
  logger.warn(
/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: [33mWARN: The obs returned by the `step()` method is not within the observation space.
  logger.warn(f"{pre} is not within the observation space.")
global_step=13, episodic_return=[10.91251]
global_step=38, episodic_return=[28.610651]
global_step=75, episodic_return=[27.435326]
global_step=102, episodic_return=[5.4052234]
global_step=113, episodic_return=[7.46734]
global_step=145, episodic_return=[26.261326]
global_step=167, episodic_return=[10.343855]
global_step=208, episodic_return=[32.33264]
global_step=223, episodic_return=[15.10476]
global_step=243, episodic_return=[21.230661]
global_step=259, episodic_return=[12.994577]
global_step=276, episodic_return=[14.5620985]
global_step=288, episodic_return=[9.228593]
global_step=329, episodic_return=[38.398487]
global_step=390, episodic_return=[67.848305]
global_step=409, episodic_return=[18.882732]
global_step=423, episodic_return=[13.0925665]
global_step=446, episodic_return=[15.711637]
global_step=465, episodic_return=[11.194533]
global_step=487, episodic_return=[13.814677]
global_step=509, episodic_return=[21.887989]
global_step=533, episodic_return=[15.6689625]
global_step=556, episodic_return=[11.10995]
global_step=570, episodic_return=[9.299522]
global_step=620, episodic_return=[31.11622]
global_step=652, episodic_return=[27.618]
global_step=671, episodic_return=[14.795697]
global_step=690, episodic_return=[15.750496]
global_step=704, episodic_return=[10.999533]
global_step=719, episodic_return=[13.189093]
global_step=739, episodic_return=[17.489927]
global_step=773, episodic_return=[18.442959]
global_step=791, episodic_return=[13.601387]
global_step=823, episodic_return=[17.120953]
global_step=846, episodic_return=[13.356247]
global_step=866, episodic_return=[16.5028]
global_step=884, episodic_return=[8.237904]
global_step=915, episodic_return=[39.05222]
global_step=926, episodic_return=[8.979667]
global_step=943, episodic_return=[8.1905155]
global_step=966, episodic_return=[10.837193]
global_step=977, episodic_return=[8.870487]
global_step=987, episodic_return=[7.7295113]
global_step=1000, episodic_return=[11.206976]
global_step=1036, episodic_return=[45.082954]
global_step=1049, episodic_return=[11.473378]
global_step=1058, episodic_return=[4.7519517]
global_step=1077, episodic_return=[15.602547]
global_step=1091, episodic_return=[8.455522]
global_step=1104, episodic_return=[12.269443]
global_step=1115, episodic_return=[9.612757]
global_step=1144, episodic_return=[20.746592]
global_step=1201, episodic_return=[59.124107]
global_step=1214, episodic_return=[10.652263]
global_step=1228, episodic_return=[11.45295]
global_step=1245, episodic_return=[13.224719]
global_step=1263, episodic_return=[11.752099]
global_step=1281, episodic_return=[13.502222]
global_step=1305, episodic_return=[24.383574]
global_step=1318, episodic_return=[11.789735]
global_step=1340, episodic_return=[11.46562]
global_step=1356, episodic_return=[10.15149]
global_step=1379, episodic_return=[30.881079]
global_step=1408, episodic_return=[10.404765]
global_step=1421, episodic_return=[11.67607]
global_step=1437, episodic_return=[9.8462305]
global_step=1452, episodic_return=[10.705885]
global_step=1470, episodic_return=[12.545941]
global_step=1494, episodic_return=[25.496634]
global_step=1531, episodic_return=[37.68815]
global_step=1542, episodic_return=[8.132014]
global_step=1558, episodic_return=[13.123883]
global_step=1578, episodic_return=[23.579687]
global_step=1604, episodic_return=[8.900353]
global_step=1624, episodic_return=[14.904622]
global_step=1643, episodic_return=[15.201219]
global_step=1664, episodic_return=[19.18242]
global_step=1690, episodic_return=[21.69039]
global_step=1728, episodic_return=[53.393856]
global_step=1745, episodic_return=[12.643121]
global_step=1798, episodic_return=[61.788296]
global_step=1812, episodic_return=[10.778318]
global_step=1828, episodic_return=[12.275214]
global_step=1844, episodic_return=[14.714555]
global_step=1858, episodic_return=[11.176002]
global_step=1879, episodic_return=[16.4808]
global_step=1902, episodic_return=[24.602652]
global_step=1978, episodic_return=[113.35282]
global_step=1999, episodic_return=[15.376908]
global_step=2033, episodic_return=[34.736435]
global_step=2105, episodic_return=[119.10404]
global_step=2116, episodic_return=[6.652383]
global_step=2144, episodic_return=[11.797904]
global_step=2212, episodic_return=[76.420784]
global_step=2227, episodic_return=[13.876787]
global_step=2249, episodic_return=[16.38469]
global_step=2270, episodic_return=[11.320691]
global_step=2297, episodic_return=[17.013807]
global_step=2319, episodic_return=[21.21184]
global_step=2329, episodic_return=[8.336202]
global_step=2339, episodic_return=[7.332041]
global_step=2362, episodic_return=[10.807482]
global_step=2374, episodic_return=[7.0916667]
global_step=2394, episodic_return=[17.686611]
global_step=2426, episodic_return=[33.148933]
global_step=2444, episodic_return=[18.563692]
global_step=2474, episodic_return=[28.454365]
global_step=2488, episodic_return=[9.844374]
global_step=2527, episodic_return=[44.57848]
global_step=2545, episodic_return=[14.623961]
global_step=2575, episodic_return=[6.7078133]
global_step=2594, episodic_return=[16.392813]
global_step=2611, episodic_return=[13.573701]
global_step=2637, episodic_return=[24.403244]
global_step=2658, episodic_return=[20.776846]
global_step=2708, episodic_return=[46.932655]
global_step=2775, episodic_return=[66.10659]
global_step=2826, episodic_return=[42.320164]
global_step=2839, episodic_return=[10.690672]
global_step=2886, episodic_return=[67.08224]
global_step=2900, episodic_return=[10.959874]
global_step=2913, episodic_return=[9.483068]
global_step=2930, episodic_return=[14.106655]
global_step=2983, episodic_return=[50.136776]
global_step=3029, episodic_return=[53.95163]
global_step=3047, episodic_return=[13.22739]
global_step=3065, episodic_return=[6.9301405]
global_step=3080, episodic_return=[9.377669]
global_step=3100, episodic_return=[15.975491]
global_step=3121, episodic_return=[16.265923]
global_step=3131, episodic_return=[6.278533]
global_step=3144, episodic_return=[11.356188]
global_step=3175, episodic_return=[39.457504]
global_step=3200, episodic_return=[10.770419]
global_step=3233, episodic_return=[22.232012]
global_step=3251, episodic_return=[12.317321]
global_step=3286, episodic_return=[35.494564]
global_step=3301, episodic_return=[10.98957]
global_step=3313, episodic_return=[8.219004]
global_step=3323, episodic_return=[6.291406]
global_step=3349, episodic_return=[10.118257]
global_step=3394, episodic_return=[63.352005]
global_step=3412, episodic_return=[17.540083]
global_step=3437, episodic_return=[21.849709]
global_step=3452, episodic_return=[10.542076]
global_step=3485, episodic_return=[19.856411]
global_step=3503, episodic_return=[15.6243]
global_step=3536, episodic_return=[46.772476]
global_step=3567, episodic_return=[18.835161]
global_step=3600, episodic_return=[27.15684]
global_step=3613, episodic_return=[10.762612]
global_step=3627, episodic_return=[10.579194]
global_step=3640, episodic_return=[11.274772]
global_step=3660, episodic_return=[14.687841]
global_step=3675, episodic_return=[12.755835]
global_step=3695, episodic_return=[13.435434]
global_step=3712, episodic_return=[8.441327]
global_step=3735, episodic_return=[8.015493]
global_step=3767, episodic_return=[16.321404]
global_step=3807, episodic_return=[45.876957]
global_step=3820, episodic_return=[12.363773]
global_step=3834, episodic_return=[7.215132]
global_step=3844, episodic_return=[7.6343102]
global_step=3886, episodic_return=[39.016968]
global_step=3895, episodic_return=[6.8335023]
global_step=3956, episodic_return=[46.633724]
global_step=3981, episodic_return=[14.114166]
global_step=3997, episodic_return=[17.027565]
global_step=4024, episodic_return=[23.346865]
global_step=4039, episodic_return=[8.068167]
global_step=4055, episodic_return=[12.842094]
global_step=4067, episodic_return=[9.050424]
global_step=4090, episodic_return=[14.300549]
global_step=4102, episodic_return=[8.155021]
global_step=4113, episodic_return=[8.280169]
global_step=4123, episodic_return=[7.093677]
global_step=4145, episodic_return=[22.305061]
global_step=4159, episodic_return=[9.3989525]
global_step=4181, episodic_return=[22.342348]
global_step=4198, episodic_return=[17.79197]
global_step=4256, episodic_return=[72.745766]
global_step=4270, episodic_return=[10.891333]
global_step=4278, episodic_return=[5.880494]
global_step=4296, episodic_return=[7.397458]
global_step=4311, episodic_return=[11.540014]
global_step=4326, episodic_return=[10.808572]
global_step=4345, episodic_return=[16.543728]
global_step=4358, episodic_return=[6.6332107]
global_step=4370, episodic_return=[11.306983]
global_step=4383, episodic_return=[9.501624]
global_step=4418, episodic_return=[33.392166]
global_step=4432, episodic_return=[12.539321]
global_step=4462, episodic_return=[28.61213]
global_step=4483, episodic_return=[15.894131]
global_step=4518, episodic_return=[25.176218]
global_step=4527, episodic_return=[6.787283]
global_step=4557, episodic_return=[25.36848]
global_step=4568, episodic_return=[6.606761]
global_step=4580, episodic_return=[8.895657]
global_step=4594, episodic_return=[8.926193]
global_step=4628, episodic_return=[29.960464]
global_step=4640, episodic_return=[10.355267]
global_step=4663, episodic_return=[17.11729]
global_step=4689, episodic_return=[22.127869]
global_step=4700, episodic_return=[8.12369]
global_step=4711, episodic_return=[7.4351897]
global_step=4734, episodic_return=[6.849504]
global_step=4754, episodic_return=[11.249021]
global_step=4764, episodic_return=[8.186752]
global_step=4784, episodic_return=[17.18664]
global_step=4808, episodic_return=[12.495514]
global_step=4824, episodic_return=[12.993621]
global_step=4849, episodic_return=[17.962973]
global_step=4860, episodic_return=[7.9686112]
global_step=4907, episodic_return=[48.710648]
global_step=4927, episodic_return=[14.221926]
global_step=4948, episodic_return=[21.101183]
global_step=4969, episodic_return=[18.61413]
global_step=4991, episodic_return=[16.83693]
Traceback (most recent call last):
  File "qflow_discrete.py", line 161, in <module>
    actions, log_prob = gflownet(torch.Tensor(obs).to(device))
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hyperpotato/qflow-discrete/qflow_model.py", line 135, in forward
    pi, logp = self.forward_once(s, a)
  File "/home/hyperpotato/qflow-discrete/qflow_model.py", line 120, in forward_once
    pi, logp = self.mlp[mlp_idx](s, tau=tau)
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hyperpotato/qflow-discrete/qflow_model.py", line 40, in forward
    x = F.gelu(self.ln1(self.fc1(s)))
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hyperpotato/anaconda3/envs/decision-transformer-gym/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x12 and 11x256)